#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘     ðŸ„ AUREON MYCELIUM NEURAL NETWORK ðŸ„                                      â•‘
â•‘                                                                               â•‘
â•‘     "The underground network where everything connects"                        â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MYCELIUM ARCHITECTURE:
======================

Just like how fungal mycelium networks connect trees in a forest,
this system connects all trading agents in a distributed intelligence network.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MYCELIUM NEURAL NETWORK                             â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚Agentâ”‚â”€â”€â”€â”‚Agentâ”‚â”€â”€â”€â”‚Agentâ”‚â”€â”€â”€â”‚Agentâ”‚â”€â”€â”€â”‚Agentâ”‚   â† HIVE 0 (Gen 0)       â”‚
â”‚   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜                          â”‚
â”‚      â”‚         â”‚         â”‚         â”‚         â”‚                              â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                â”‚    SYNAPSES       â”‚                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚         â”‚   NEURON    â”‚â”€â”€â”€â”€â”€â”‚   NEURON    â”‚    â† HIDDEN LAYER               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                â”‚                   â”‚                                        â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                          â”‚                                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                            â”‚
â”‚                    â”‚  QUEEN    â”‚    â† DECISION NODE                         â”‚
â”‚                    â”‚  NEURON   â”‚                                            â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚                          â”‚                                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                            â”‚
â”‚                    â”‚  SIGNAL   â”‚    â† BUY / SELL / HOLD                     â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚                                                                             â”‚
â”‚   When profit > threshold â†’ SPAWN NEW HIVE (budding reproduction)           â”‚
â”‚   90% compounds â†’ 10% harvests for new hive                                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

NEURAL COMPONENTS:
- Agent: Individual trading entity with its own strategy bias
- Synapse: Connection between agents that carries signal weight
- Neuron: Processing node that aggregates signals
- Queen Neuron: Final decision maker

MYCELIUM FEATURES:
- Distributed intelligence (no single point of failure)
- Signal propagation through network
- Adaptive weights (learning from successful trades)
- Spawning new hives (network growth)

Gary Leckey & GitHub Copilot | November 2025
"""

from aureon_baton_link import link_system as _baton_link; _baton_link(__name__)
import math
import time
import random
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from collections import deque
from aureon_memory_core import memory  # ðŸ§  MEMORY CORE INTEGRATION

# ðŸ”±ðŸ”® ENHANCED PROBABILITY NEXUS - Lazy load to avoid circular imports
ENHANCED_NEXUS_AVAILABLE = False
EnhancedProbabilityNexus = None
ProfitFilter = None
CompoundingEngine = None
AureonProbabilityNexus = None

def _lazy_load_probability_nexus():
    """Lazy load probability nexus to avoid circular imports"""
    global ENHANCED_NEXUS_AVAILABLE, EnhancedProbabilityNexus, ProfitFilter, CompoundingEngine, AureonProbabilityNexus
    if EnhancedProbabilityNexus is not None:
        return ENHANCED_NEXUS_AVAILABLE
    try:
        from aureon_probability_nexus import (
            EnhancedProbabilityNexus as _EnhancedProbabilityNexus,
            ProfitFilter as _ProfitFilter,
            CompoundingEngine as _CompoundingEngine,
            AureonProbabilityNexus as _AureonProbabilityNexus,
        )
        EnhancedProbabilityNexus = _EnhancedProbabilityNexus
        ProfitFilter = _ProfitFilter
        CompoundingEngine = _CompoundingEngine
        AureonProbabilityNexus = _AureonProbabilityNexus
        ENHANCED_NEXUS_AVAILABLE = True
        return True
    except ImportError:
        ENHANCED_NEXUS_AVAILABLE = False
        return False

# ðŸ’Ž PROBABILITY ULTIMATE INTELLIGENCE - 95% Accuracy Pattern Learning
try:
    from probability_ultimate_intelligence import (
        get_ultimate_intelligence, ultimate_predict, record_ultimate_outcome,
        UltimatePrediction
    )
    ULTIMATE_INTELLIGENCE_AVAILABLE = True
    print("ðŸ’Ž Mycelium: Ultimate Intelligence WIRED! (95% accuracy)")
except ImportError:
    ULTIMATE_INTELLIGENCE_AVAILABLE = False
    print("âš ï¸ Mycelium: Ultimate Intelligence not available")

# â³ðŸ”® TIMELINE ORACLE - Lazy load to avoid circular imports
TIMELINE_ORACLE_AVAILABLE = False
TimelineOracle = None
TimelineBranch = None
TimelineAction = None
timeline_select = None
timeline_validate = None
get_timeline_oracle = None
_timeline_oracle_loaded = False

def _lazy_load_timeline_oracle():
    """Lazy load timeline oracle to avoid circular imports with aureon_mycelium"""
    global TIMELINE_ORACLE_AVAILABLE, TimelineOracle, TimelineBranch, TimelineAction
    global timeline_select, timeline_validate, get_timeline_oracle, _timeline_oracle_loaded
    if _timeline_oracle_loaded:
        return TIMELINE_ORACLE_AVAILABLE
    _timeline_oracle_loaded = True
    try:
        from aureon_timeline_oracle import (
            TimelineOracle as _TimelineOracle,
            TimelineBranch as _TimelineBranch,
            TimelineAction as _TimelineAction,
            timeline_select as _timeline_select,
            timeline_validate as _timeline_validate,
            get_timeline_oracle as _get_timeline_oracle
        )
        TimelineOracle = _TimelineOracle
        TimelineBranch = _TimelineBranch
        TimelineAction = _TimelineAction
        timeline_select = _timeline_select
        timeline_validate = _timeline_validate
        get_timeline_oracle = _get_timeline_oracle
        TIMELINE_ORACLE_AVAILABLE = True
        print("â³ðŸ”® Mycelium: Timeline Oracle WIRED! (7-day vision)")
        return True
    except ImportError:
        TIMELINE_ORACLE_AVAILABLE = False
        return False

# ðŸ‘‘ðŸ„ QUEEN HIVE MIND - Lazy load to avoid circular imports
QUEEN_HIVE_MIND_AVAILABLE = False
QueenHiveMind = None
QueenWisdom = None
get_queen = None
_queen_loaded = False

def _lazy_load_queen():
    """Lazy load Queen Hive Mind to avoid circular imports"""
    global QUEEN_HIVE_MIND_AVAILABLE, QueenHiveMind, QueenWisdom, get_queen, _queen_loaded
    if _queen_loaded:
        return QUEEN_HIVE_MIND_AVAILABLE
    _queen_loaded = True
    try:
        from aureon_queen_hive_mind import (
            QueenHiveMind as _QueenHiveMind,
            QueenWisdom as _QueenWisdom,
            get_queen as _get_queen
        )
        QueenHiveMind = _QueenHiveMind
        QueenWisdom = _QueenWisdom
        get_queen = _get_queen
        QUEEN_HIVE_MIND_AVAILABLE = True
        print("ðŸ‘‘ðŸ„ Mycelium: Queen Hive Mind WIRED! (The Dreaming Queen)")
        return True
    except ImportError:
        QUEEN_HIVE_MIND_AVAILABLE = False
        return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONSTANTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHI = (1 + math.sqrt(5)) / 2
PRIMES = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYNAPSE - Connection between nodes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class Synapse:
    """
    A synapse connects two nodes in the network.
    Weight determines signal strength (learning happens here).
    """
    source_id: str
    target_id: str
    weight: float = 1.0
    plasticity: float = 0.1  # How fast the synapse learns
    last_signal: float = 0.0
    activation_count: int = 0
    
    def transmit(self, signal: float) -> float:
        """Transmit signal through synapse with weight modulation"""
        self.last_signal = signal
        self.activation_count += 1
        return signal * self.weight
    
    def strengthen(self, reward: float):
        """Hebbian learning: strengthen successful connections"""
        delta = self.plasticity * reward * self.last_signal
        self.weight = max(0.1, min(2.0, self.weight + delta))
    
    def weaken(self, penalty: float):
        """Weaken unsuccessful connections"""
        delta = self.plasticity * penalty * self.last_signal
        self.weight = max(0.1, self.weight - delta)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEURON - Processing node
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class Neuron:
    """
    A neuron aggregates signals from synapses and produces output.
    """
    id: str
    bias: float = 0.0
    activation: float = 0.0
    inputs: List[float] = field(default_factory=list)
    
    def activate(self, inputs: List[float]) -> float:
        """
        Activation function: tanh for bounded output [-1, 1]
        Positive = BUY bias, Negative = SELL bias
        """
        self.inputs = inputs
        total = sum(inputs) + self.bias
        self.activation = math.tanh(total)
        return self.activation
    
    def sigmoid(self, x: float) -> float:
        """Sigmoid activation for probability output [0, 1]"""
        return 1 / (1 + math.exp(-x))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AGENT - Trading entity
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class Agent:
    """
    An individual trading agent within a hive.
    Each agent has its own equity and strategy bias.
    """
    id: int
    hive_id: str
    equity: float
    start_equity: float
    trades: int = 0
    wins: int = 0
    prime_idx: int = 0
    symbol: str = "BTCUSDT"
    position_open: bool = False
    last_signal: float = 0.0
    
    def __post_init__(self):
        self.prime_idx = self.id % len(PRIMES)
    
    def compute_signal(self, market_data: Dict[str, float], probability_bias: float = 0.0) -> float:
        """
        Compute trading signal based on market data.
        Returns value in [-1, 1]: negative = SELL, positive = BUY
        """
        # Each agent has a unique perspective based on prime number
        prime = PRIMES[self.prime_idx]
        
        # Factors
        momentum = market_data.get("momentum", 0)
        volatility = market_data.get("volatility", 0.5)
        trend = market_data.get("trend", 0)
        
        # Unique bias per agent
        bias = math.sin(prime * 0.1) * 0.3
        
        # Combine factors - NOW WITH PROBABILITY BIAS!
        # If probability is high, we trust momentum more and ignore volatility
        signal = momentum * 0.4 + trend * 0.3 + bias + (1 - volatility) * 0.2
        
        # ðŸŽ¯ ONE GOAL INJECTION: If probability is validated, boost signal!
        if probability_bias != 0:
            signal += probability_bias * 0.5
            
        self.last_signal = max(-1, min(1, signal))
        
        return self.last_signal
    
    def execute_trade(self, signal: float, price: float) -> Dict[str, Any]:
        """
        Execute a trade based on signal.
        Returns trade result.
        """
        self.trades += 1
        
        # Position sizing based on prime number (0.02 to 0.97% of equity)
        prime = PRIMES[self.prime_idx]
        self.prime_idx = (self.prime_idx + 1) % len(PRIMES)
        position_pct = prime * 0.01
        
        # Simulate trade (in real system, would call Binance)
        # For now, use signal as proxy for return direction
        expected_return = signal * 0.002  # Â±0.2% expected move
        noise = (random.random() - 0.5) * 0.001  # Â±0.05% noise
        actual_return = expected_return + noise
        
        pnl = self.equity * position_pct * actual_return
        self.equity += pnl
        
        if pnl > 0:
            self.wins += 1
        
        return {
            "agent_id": self.id,
            "signal": signal,
            "position_pct": position_pct,
            "pnl": pnl,
            "new_equity": self.equity,
            "win": pnl > 0
        }
    
    def get_profit(self) -> float:
        """Get profit since start"""
        return self.equity - self.start_equity
    
    def get_win_rate(self) -> float:
        """Get win rate"""
        return self.wins / self.trades if self.trades > 0 else 0.0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HIVE - Collection of agents
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Hive:
    """
    A hive contains multiple agents working together.
    Implements the 10-9-1 revenue model.
    """
    
    def __init__(self, hive_id: str, generation: int, agent_count: int, 
                 equity_per_agent: float, target_per_agent: float):
        self.id = hive_id
        self.generation = generation
        self.agent_count = agent_count
        self.start_equity_per_agent = equity_per_agent
        self.target_per_agent = target_per_agent
        
        # Create agents
        self.agents: List[Agent] = []
        for i in range(agent_count):
            agent = Agent(
                id=i,
                hive_id=hive_id,
                equity=equity_per_agent,
                start_equity=equity_per_agent
            )
            self.agents.append(agent)
        
        # Metrics
        self.trades = 0
        self.harvested_capital = 0.0
        self.age = 0
        self.successful_agents = 0
        
        # Neural components
        self.neuron = Neuron(id=f"hive_{hive_id}_neuron")
        self.synapses: List[Synapse] = []
        self._create_synapses()
    
    def _create_synapses(self):
        """Create synapses between agents and hive neuron"""
        for agent in self.agents:
            synapse = Synapse(
                source_id=f"agent_{agent.id}",
                target_id=self.neuron.id,
                weight=1.0 / len(self.agents)  # Equal initial weights
            )
            self.synapses.append(synapse)
    
    def step(self, market_data: Dict[str, float], probability_bias: float = 0.0) -> Dict[str, Any]:
        """
        Execute one trading step for all agents.
        """
        self.age += 1
        
        # Collect signals from all agents
        signals = []
        for agent in self.agents:
            if agent.equity > 0:
                signal = agent.compute_signal(market_data, probability_bias)
                signals.append(signal)
        
        # Transmit through synapses
        transmitted = []
        for i, synapse in enumerate(self.synapses):
            if i < len(signals):
                transmitted.append(synapse.transmit(signals[i]))
        
        # Aggregate in neuron
        hive_signal = self.neuron.activate(transmitted)
        
        # Execute trades for agents with strong signals
        results = []
        for agent in self.agents:
            if agent.equity > 0 and agent.equity < self.target_per_agent:
                price = market_data.get("price", 95000)
                result = agent.execute_trade(hive_signal, price)
                results.append(result)
                self.trades += 1
                
                # Update synapse weights based on result
                synapse = self.synapses[agent.id] if agent.id < len(self.synapses) else None
                if synapse:
                    if result["win"]:
                        synapse.strengthen(0.1)
                    else:
                        synapse.weaken(0.05)
                
                # Check if agent reached target
                if agent.equity >= self.target_per_agent:
                    self.successful_agents += 1
        
        return {
            "hive_id": self.id,
            "hive_signal": hive_signal,
            "trades": len(results),
            "results": results
        }
    
    def harvest_capital(self) -> float:
        """
        10-9-1 model: Extract 10% of profit for new hive spawning.
        """
        total_equity = self.get_total_equity()
        start_total = self.start_equity_per_agent * self.agent_count
        total_profit = max(0, total_equity - start_total)
        
        harvest_amount = total_profit * 0.10  # 10% harvest
        self.harvested_capital += harvest_amount
        
        # Deduct harvest from agents proportionally
        for agent in self.agents:
            agent_profit = agent.get_profit()
            if agent_profit > 0:
                agent_harvest = agent_profit * 0.10
                agent.equity -= agent_harvest
        
        return harvest_amount
    
    def get_total_equity(self) -> float:
        """Get total equity across all agents"""
        return sum(agent.equity for agent in self.agents)
    
    def get_profit_multiplier(self) -> float:
        """Get profit multiplier (current / start)"""
        start_total = self.start_equity_per_agent * self.agent_count
        return self.get_total_equity() / start_total if start_total > 0 else 1.0
    
    def can_split(self) -> bool:
        """Check if hive is ready to spawn a new hive"""
        return self.successful_agents >= self.agent_count * 0.5
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get hive metrics"""
        stage = "mature" if self.successful_agents == self.agent_count else \
                "ready_to_split" if self.can_split() else "growing"
        
        return {
            "id": self.id,
            "generation": self.generation,
            "agents": self.agent_count,
            "equity": self.get_total_equity(),
            "harvested_capital": self.harvested_capital,
            "trades": self.trades,
            "successful_agents": self.successful_agents,
            "stage": stage,
            "age": self.age,
            "profit_multiplier": self.get_profit_multiplier()
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MYCELIUM NETWORK - The full neural network
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MyceliumNetwork:
    """
    The Mycelium Neural Network - connects all hives in a distributed intelligence.
    
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  ðŸŽ¯ ONE GOAL: GROW NET PROFIT - FAST! ðŸŽ¯                                      â•‘
    â•‘                                                                               â•‘
    â•‘  Every decision, every signal, every spawn - serves ONE purpose:             â•‘
    â•‘  MAXIMIZE NET PROFIT GROWTH AS FAST AS POSSIBLE                               â•‘
    â•‘                                                                               â•‘
    â•‘  The Mycelium doesn't care about:                                             â•‘
    â•‘  - Being "safe" (we have other systems for risk)                              â•‘
    â•‘  - Waiting around (time = money)                                              â•‘
    â•‘  - Small gains (we compound AGGRESSIVELY)                                     â•‘
    â•‘                                                                               â•‘
    â•‘  The Mycelium ONLY cares about:                                               â•‘
    â•‘  - NET PROFIT (after ALL fees)                                                â•‘
    â•‘  - GROWTH RATE (faster is better)                                             â•‘
    â•‘  - COMPOUNDING (every penny back in)                                          â•‘
    â•‘                                                                               â•‘
    â•‘  ðŸ”±ðŸ”® NOW ENHANCED WITH 100% WIN RATE PROFIT FILTER ðŸ”®ðŸ”±                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Like fungal mycelium in a forest, this network:
    - Distributes resources (capital) where needed
    - Shares information (market signals)
    - Enables collective decision making
    - Grows through spawning new hives
    - ðŸ”± NOW: Only takes trades with guaranteed profitable exits!
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ðŸŽ¯ THE ONE GOAL - ENCODED INTO EVERY FIBER
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ONE_GOAL = "GROW_NET_PROFIT_FAST"
    TARGET_EQUITY = 1_000_000.0  # ðŸŽ¯ THE MILLION

    def acknowledge_war_band(self):
        """
        Connects the Apache War Band to the Mycelium Network.
        The War Band acts as an autonomous special forces unit.
        """
        logger.info("ðŸ„ Mycelium Network: Connected to Apache War Band (Autonomous Unit)")
        # The War Band operates as a specialized hunter-killer node
        # It feeds profit back into the ecosystem, fueling the mycelial growth.
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ðŸŽ¯ S5 MATHEMATICAL CONSTANTS - THE PATH TO $1,000,000
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # S5 = Speed Ã— Scale Ã— Smart Ã— Systematic Ã— Sustainable
    TARGET_MILLION = 1_000_000.0
    GROWTH_AGGRESSION = 0.9999  # 99.99% MAXIMUM AGGRESSION - SPEED TO MILLION
    COMPOUND_RATE = 0.995      # 99.5% of profits compound back (aggressive!)
    # Global epsilon profit policy: accept any net-positive edge after costs.
    MIN_PROFIT_TARGET = 0.0001
    
    # S5 Velocity Constants
    S5_VELOCITY_THRESHOLD = 100.0  # $/hour threshold for acceleration
    S5_ACCELERATION_MULTIPLIER = 1.5  # Boost factor when velocity is high
    S5_OPTIMAL_CONVERSIONS_PER_HOUR = 1000  # Target conversion frequency
    S5_KELLY_FRACTION = 0.25  # Kelly criterion fraction for sizing
    
    # âš¡ SPEED OPTIMIZATION CONSTANTS
    S5_CACHE_TTL = 60  # Cache TTL in seconds (1 minute)
    S5_BATCH_SIZE = 100  # Batch size for bulk operations
    S5_HOT_PATH_THRESHOLD = 10  # Executions to mark path as "hot"
    
    def __init__(self, initial_capital: float, agents_per_hive: int = 5,
                 target_multiplier: float = 2.0, leverage: float = 1.0,
                 target_equity: float | None = None):
        self.initial_capital = initial_capital
        self.agents_per_hive = agents_per_hive
        self.target_multiplier = target_multiplier
        
        # ðŸŽ¯ THE GOAL - Track it obsessively
        self.starting_equity = initial_capital
        self.peak_equity = initial_capital
        self.net_profit_total = 0.0
        self.profit_rate_per_hour = 0.0
        self.start_time = time.time()

        # ðŸŽ¯ OPTIONAL ABSOLUTE TARGET (e.g. 100K)
        self.target_equity: float | None = None
        try:
            if target_equity is not None and float(target_equity) > 0:
                self.target_equity = float(target_equity)
        except Exception:
            self.target_equity = None
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ðŸ”„ CONVERSION PROFIT METRICS - Track every conversion's net profit
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.conversion_metrics = {
            'total_conversions': 0,
            'successful_conversions': 0,
            'failed_conversions': 0,
            'total_conversion_profit': 0.0,
            'total_conversion_fees': 0.0,
            'net_conversion_profit': 0.0,
            'conversion_profit_rate_per_hour': 0.0,
            'best_path': None,
            'best_path_profit': 0.0,
            'worst_path': None,
            'worst_path_loss': 0.0,
            'path_performance': {},  # path_key -> {profit, count, avg_profit}
            'asset_performance': {},  # asset -> {profit, conversions}
            'exchange_performance': {},  # exchange -> {profit, conversions}
            # S5 VELOCITY METRICS
            'velocity_per_hour': 0.0,
            'acceleration': 0.0,
            'time_to_million': float('inf'),
            'conversions_per_hour': 0.0,
        }
        self.conversion_history: deque = deque(maxlen=10000)  # 10K for velocity analysis
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ðŸŽ¯ S5 AGGRESSIVE CONVERSION GOALS - PATH TO $1,000,000
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.conversion_goals = {
            'target_net_profit': self.TARGET_MILLION,  # $1,000,000 target
            'min_profit_per_conversion': 0.0001,  # Minimum $0.0001 (micro profits add up!)
            'target_daily_conversions': 10000,  # 10K conversions/day
            'target_hourly_conversions': 500,  # ~500/hour
            'target_daily_profit': 1000.0,  # Target $1000/day (365 days = $365K)
            'target_hourly_profit': 50.0,  # Target $50/hour
            'max_single_hop_fee_pct': 0.001,  # 0.1% max fee per hop (aggressive)
            'velocity_target': 100.0,  # $/hour velocity target
            'acceleration_target': 10.0,  # $/hourÂ² acceleration target
        }
        
        # S5 Adaptive State
        self.s5_state = {
            'phase': 'BOOTSTRAP',  # BOOTSTRAP -> GROWTH -> SCALE -> COMPOUND -> MILLION
            'velocity_history': deque(maxlen=100),
            'acceleration_history': deque(maxlen=100),
            'phase_transitions': [],
            'optimal_paths_cache': {},  # Cached optimal paths
            'path_velocity': {},  # Velocity per path
        }
        
        # âš¡ S5 SPEED CACHES - Hot path optimization
        self._s5_score_cache = {}  # path_key -> (score, timestamp)
        self._s5_hot_paths = set()  # Frequently used paths
        self._s5_last_batch_time = time.time()
        self._s5_pending_updates = []  # Batch updates queue
        
        # Network state
        self.hives: List[Hive] = []
        self.generation = 0
        self.total_harvested = 0.0
        self.split_events: List[Dict] = []
        self.step_count = 0

        # Ecosystem connectivity map (populated by orchestrators like MultiverseLiveEngine)
        self.connection_map: Dict[str, Any] = {}

        # Governing metrics (profit/portfolio health) pushed by orchestrators
        self.governing_metrics: Dict[str, Any] = {}
        self.metrics_history: deque = deque(maxlen=240)  # ~240 cycles worth of snapshots
        
        # Queen neuron - final decision aggregator
        self.queen_neuron = Neuron(id="queen", bias=0.0)
        self.hive_synapses: List[Synapse] = []
        
        # ï¿½ðŸ„ QUEEN HIVE MIND - The Dreaming Queen
        self.queen_hive_mind = None
        self.queen_wisdom_cache: Optional[Any] = None
        if QUEEN_HIVE_MIND_AVAILABLE:
            try:
                # Don't create a new queen, get the global singleton
                # The Queen will wire us, not the other way around
                logger.info("ðŸ‘‘ðŸ„ Mycelium ready to receive wisdom from Queen Hive Mind")
            except Exception as e:
                logger.warning(f"âš ï¸ Queen Hive Mind not available: {e}")
        
        # ï¿½ðŸ”±ðŸ”® ENHANCED PROBABILITY NEXUS - 100% WIN RATE INTEGRATION ðŸ”®ðŸ”±
        self.enhanced_nexus = None
        self.profit_filter = None
        self.compounding_engine = None
        self.leverage = leverage
        
        if ENHANCED_NEXUS_AVAILABLE:
            try:
                self.enhanced_nexus = EnhancedProbabilityNexus(
                    exchange='binance',
                    leverage=leverage,
                    starting_balance=initial_capital
                )
                self.profit_filter = ProfitFilter(fee_rate=0.001)  # Binance 0.10%
                self.compounding_engine = CompoundingEngine(
                    starting_balance=initial_capital, 
                    leverage=leverage
                )
                logger.info("ðŸ”±ðŸ”® Enhanced Probability Nexus WIRED to Mycelium!")
                logger.info(f"   Leverage: {leverage}x | Profit Filter: ACTIVE")
            except Exception as e:
                logger.warning(f"âš ï¸ Enhanced Nexus init failed: {e}")
        
        # Create root hive
        self._spawn_hive(initial_capital)
        
        logger.info(f"ðŸ„ Mycelium Network initialized with ${initial_capital:.2f}")
        logger.info(f"ðŸŽ¯ ONE GOAL ACTIVE: {self.ONE_GOAL} | Aggression: {self.GROWTH_AGGRESSION*100}%")

    def update_connection_map(self, connection_map: Dict[str, Any]) -> None:
        """Store latest ecosystem connectivity graph so Mycelium can reason over all connections."""
        if isinstance(connection_map, dict):
            self.connection_map = connection_map

    def update_governing_metrics(self, metrics: Dict[str, Any]) -> None:
        """Ingest governing metrics (net profit, win rate, drawdown, cash/positions) from the live engine."""
        if not isinstance(metrics, dict):
            return
        self.governing_metrics = metrics
        self.metrics_history.append(metrics)

    def get_growth_governor(self) -> Dict[str, Any]:
        """Convert governing metrics into a growth/portfolio governor for the ONE GOAL."""
        m = self.governing_metrics or {}

        def _f(key: str, default: float = 0.0) -> float:
            try:
                return float(m.get(key, default) or default)
            except Exception:
                return float(default)

        total_equity = _f("total_equity", 0.0)
        total_cash = _f("total_cash", 0.0)
        realized_pnl_total = _f("realized_pnl_total", 0.0)
        win_rate = _f("win_rate", 0.0)
        drawdown_pct = _f("drawdown_pct", 0.0)
        positions_count = int(m.get("positions_count", 0) or 0)

        # Baseline defaults: conservative but active
        governor: Dict[str, Any] = {
            "allow_entries": True,
            "entry_budget_scale": 1.0,
            "entry_confidence_floor": 0.4,
            "max_entries_per_cycle": 3,
            "max_positions_total": 12,
            "reason": "baseline",
        }

        # Hard constraint: if basically no cash, donâ€™t try to expand
        if total_cash < 1.0:
            governor.update({
                "allow_entries": False,
                "entry_budget_scale": 0.0,
                "max_entries_per_cycle": 0,
                "reason": "cash_low",
            })
            return governor

        # Risk control: if drawdown is material, tighten up
        if drawdown_pct >= 10.0:
            governor.update({
                "allow_entries": True,
                "entry_budget_scale": 0.3,
                "entry_confidence_floor": 0.8,
                "max_entries_per_cycle": 1,
                "max_positions_total": max(4, min(positions_count, 8)),
                "reason": "drawdown_control",
            })
            return governor

        # Profit expansion: if weâ€™re net-positive and winning, expand portfolio pressure
        if realized_pnl_total > 0 and win_rate >= 0.55:
            # Scale with equity: larger equity can support more positions
            max_pos = 12
            if total_equity >= 250:
                max_pos = 18
            if total_equity >= 1000:
                max_pos = 25

            governor.update({
                "allow_entries": True,
                "entry_budget_scale": 1.25,
                "entry_confidence_floor": 0.35,
                "max_entries_per_cycle": 5,
                "max_positions_total": max_pos,
                "reason": "expand_profitable",
            })
            return governor

        # If win-rate is low, tighten entries until the system proves itself
        if win_rate > 0 and win_rate < 0.45:
            governor.update({
                "allow_entries": True,
                "entry_budget_scale": 0.5,
                "entry_confidence_floor": 0.75,
                "max_entries_per_cycle": 1,
                "max_positions_total": 8,
                "reason": "winrate_low",
            })
            return governor

        return governor

    def set_target_equity(self, target_equity: float | None) -> None:
        """Set/override an absolute network goal (optional)."""
        try:
            if target_equity is None:
                self.target_equity = None
            else:
                v = float(target_equity)
                self.target_equity = v if v > 0 else None
        except Exception:
            self.target_equity = None
    
    def _spawn_hive(self, capital: float, parent_generation: int = -1) -> Hive:
        """Spawn a new hive with given capital"""
        new_generation = parent_generation + 1
        hive_id = f"hive_{new_generation}_{len(self.hives)}"
        
        equity_per_agent = capital / self.agents_per_hive
        target_per_agent = equity_per_agent * self.target_multiplier
        
        hive = Hive(
            hive_id=hive_id,
            generation=new_generation,
            agent_count=self.agents_per_hive,
            equity_per_agent=equity_per_agent,
            target_per_agent=target_per_agent
        )
        
        self.hives.append(hive)
        
        # Create synapse from hive to queen
        synapse = Synapse(
            source_id=hive_id,
            target_id="queen",
            weight=1.0 / (len(self.hives) + 1)
        )
        self.hive_synapses.append(synapse)
        
        # Update all synapse weights for balance
        for syn in self.hive_synapses:
            syn.weight = 1.0 / len(self.hive_synapses)
        
        self.generation = max(self.generation, new_generation)
        
        logger.info(f"ðŸ Spawned {hive_id} (Gen {new_generation}) with ${capital:.4f}")
        
        return hive
    
    def step(self, market_data: Dict[str, float], probability_map: Dict[str, float] = None) -> Dict[str, Any]:
        """
        Execute one step of the mycelium network.
        All hives process market data and contribute to collective decision.
        """
        self.step_count += 1
        
        # ðŸŒŠ SURGE WINDOW CHECK
        # If we are in a surge window, we boost the signal transmission
        surge_active = memory.is_surge_window_active()
        
        # Extract probability bias from map (average of all validated signals)
        prob_bias = 0.0
        if probability_map:
            # Use the average probability of high-confidence signals as a global bias
            high_conf_probs = [p for p in probability_map.values() if p > 0.7]
            if high_conf_probs:
                prob_bias = (sum(high_conf_probs) / len(high_conf_probs)) * 2 - 1 # Map [0.7, 1.0] to [0.4, 1.0] approx
        
        # Step all hives and collect signals
        hive_signals = []
        all_results = []
        
        for hive in self.hives:
            result = hive.step(market_data, probability_bias=prob_bias)
            signal = result["hive_signal"]
            
            # Boost signal during surge
            if surge_active:
                signal *= 1.5
                
            hive_signals.append(signal)
            all_results.append(result)
        
        # Transmit through hive synapses to queen
        transmitted = []
        for i, synapse in enumerate(self.hive_synapses):
            if i < len(hive_signals):
                # Synapses learn faster during surge
                if surge_active:
                    synapse.plasticity = 0.2 # Double learning rate
                else:
                    synapse.plasticity = 0.1
                    
                transmitted.append(synapse.transmit(hive_signals[i]))
        
        # Queen neuron makes final decision
        queen_signal = self.queen_neuron.activate(transmitted)
        
        # Check for hive splitting (ONLY during surge or high profit)
        if surge_active or self.get_profit_multiplier() > 1.1:
            self._check_splits()
        
        # Periodic harvesting
        if self.step_count % 10 == 0:
            self._harvest_all()
        
        return {
            "step": self.step_count,
            "queen_signal": queen_signal,
            "hive_count": len(self.hives),
            "total_equity": self.get_total_equity(),
            "generation": self.generation,
            "results": all_results,
            "surge_active": surge_active
        }
    
    def _check_splits(self):
        """Check if any hives are ready to split"""
        for hive in self.hives:
            if hive.can_split() and hive.harvested_capital > 0:
                # Spawn new hive with harvested capital
                spawn_capital = hive.harvested_capital
                hive.harvested_capital = 0
                
                new_hive = self._spawn_hive(spawn_capital, hive.generation)
                
                self.split_events.append({
                    "step": self.step_count,
                    "parent_hive": hive.id,
                    "new_hive": new_hive.id,
                    "spawn_capital": spawn_capital
                })
    
    def _harvest_all(self):
        """Harvest capital from all hives"""
        for hive in self.hives:
            harvested = hive.harvest_capital()
            self.total_harvested += harvested
    
    def get_total_equity(self) -> float:
        """Get total equity across all hives"""
        return sum(hive.get_total_equity() for hive in self.hives)
    
    def get_total_agents(self) -> int:
        """Get total agent count"""
        return sum(hive.agent_count for hive in self.hives)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ðŸŽ¯ THE ONE GOAL - PROFIT TRACKING & ACCELERATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def record_trade_profit(self, net_profit: float, trade_data: Dict = None):
        """
        Record a trade profit - THE MOST IMPORTANT METHOD!
        Every penny of net profit gets tracked and compounds.
        """
        self.net_profit_total += net_profit
        
        # Update equity tracking
        current_equity = self.get_total_equity()
        self.peak_equity = max(self.peak_equity, current_equity)
        
        # Calculate profit rate ($ per hour)
        elapsed_hours = max((time.time() - self.start_time) / 3600, 0.001)
        self.profit_rate_per_hour = self.net_profit_total / elapsed_hours
        
        # Compound profits back into the network (95%)
        compound_amount = net_profit * self.COMPOUND_RATE
        if compound_amount > 0 and self.hives:
            # Distribute to strongest performing hive's agents
            best_hive = max(self.hives, key=lambda h: h.get_profit_multiplier())
            for agent in best_hive.agents:
                agent.equity += compound_amount / len(best_hive.agents)
        
        logger.info(f"ðŸŽ¯ NET PROFIT: ${net_profit:+.4f} | Total: ${self.net_profit_total:.2f} | Rate: ${self.profit_rate_per_hour:.2f}/hr")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ðŸ”„ CONVERSION PROFIT TRACKING - The Labyrinth's Profit Map
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def record_conversion_profit(self, conversion_data: Dict[str, Any]) -> None:
        """
        Record profit from a conversion - CRITICAL FOR LABYRINTH LEARNING!
        
        Args:
            conversion_data: {
                'from_asset': str,
                'to_asset': str,
                'exchange': str,
                'path': List[Dict],  # The conversion path used
                'input_amount': float,
                'output_amount': float,
                'input_value_usd': float,
                'output_value_usd': float,
                'fees': float,
                'net_profit': float,
                'success': bool,
                'hops': int,
            }
        """
        from_asset = conversion_data.get('from_asset', '')
        to_asset = conversion_data.get('to_asset', '')
        exchange = conversion_data.get('exchange', '')
        path = conversion_data.get('path', [])
        net_profit = float(conversion_data.get('net_profit', 0) or 0)
        fees = float(conversion_data.get('fees', 0) or 0)
        success = conversion_data.get('success', False)
        hops = conversion_data.get('hops', 1)
        
        # Update basic metrics
        self.conversion_metrics['total_conversions'] += 1
        if success:
            self.conversion_metrics['successful_conversions'] += 1
        else:
            self.conversion_metrics['failed_conversions'] += 1
        
        self.conversion_metrics['total_conversion_profit'] += max(net_profit, 0)
        self.conversion_metrics['total_conversion_fees'] += fees
        self.conversion_metrics['net_conversion_profit'] += net_profit
        
        # Update profit rate
        elapsed_hours = max((time.time() - self.start_time) / 3600, 0.001)
        self.conversion_metrics['conversion_profit_rate_per_hour'] = \
            self.conversion_metrics['net_conversion_profit'] / elapsed_hours
        
        # Track path performance (key = "FROMâ†’TO" or "FROMâ†’INTERâ†’TO")
        path_key = f"{from_asset}â†’{to_asset}"
        if path and len(path) > 1:
            intermediates = [step.get('to', '') for step in path[:-1]]
            path_key = f"{from_asset}â†’{'â†’'.join(intermediates)}â†’{to_asset}"
        
        if path_key not in self.conversion_metrics['path_performance']:
            self.conversion_metrics['path_performance'][path_key] = {
                'profit': 0.0, 'count': 0, 'avg_profit': 0.0, 'wins': 0, 'losses': 0
            }
        
        pm = self.conversion_metrics['path_performance'][path_key]
        pm['profit'] += net_profit
        pm['count'] += 1
        pm['avg_profit'] = pm['profit'] / pm['count'] if pm['count'] > 0 else 0
        if net_profit > 0:
            pm['wins'] += 1
        else:
            pm['losses'] += 1
        
        # Track best/worst paths
        if net_profit > self.conversion_metrics['best_path_profit']:
            self.conversion_metrics['best_path_profit'] = net_profit
            self.conversion_metrics['best_path'] = path_key
        if net_profit < self.conversion_metrics['worst_path_loss']:
            self.conversion_metrics['worst_path_loss'] = net_profit
            self.conversion_metrics['worst_path'] = path_key
        
        # Track asset performance
        for asset in [from_asset, to_asset]:
            if asset not in self.conversion_metrics['asset_performance']:
                self.conversion_metrics['asset_performance'][asset] = {'profit': 0.0, 'conversions': 0}
            self.conversion_metrics['asset_performance'][asset]['profit'] += net_profit / 2
            self.conversion_metrics['asset_performance'][asset]['conversions'] += 1
        
        # Track exchange performance
        if exchange not in self.conversion_metrics['exchange_performance']:
            self.conversion_metrics['exchange_performance'][exchange] = {'profit': 0.0, 'conversions': 0}
        self.conversion_metrics['exchange_performance'][exchange]['profit'] += net_profit
        self.conversion_metrics['exchange_performance'][exchange]['conversions'] += 1
        
        # Store in history
        self.conversion_history.append({
            'timestamp': time.time(),
            **conversion_data,
            'path_key': path_key,
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ðŸš€ S5 VELOCITY & ACCELERATION TRACKING - THE MATH TO $1,000,000
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self._update_s5_velocity(net_profit, path_key)
        
        # Also record to main profit tracker (conversions are profits too!)
        if success and net_profit > 0:
            self.record_trade_profit(net_profit, {
                'source': 'conversion',
                'from': from_asset,
                'to': to_asset,
                'exchange': exchange,
                'hops': hops,
            })
        
        # Log with S5 metrics
        status = "âœ…" if success and net_profit >= 0 else "âš ï¸"
        velocity = self.conversion_metrics.get('velocity_per_hour', 0)
        ttm = self.conversion_metrics.get('time_to_million', float('inf'))
        ttm_str = f"{ttm:.1f}h" if ttm < 10000 else "âˆž"
        
        logger.info(
            f"ðŸ”„ CONVERSION: {status} {path_key} | "
            f"Net: ${net_profit:+.4f} | Total: ${self.conversion_metrics['net_conversion_profit']:.2f} | "
            f"V: ${velocity:.2f}/hr | TTM: {ttm_str}"
        )
    
    def _update_s5_velocity(self, net_profit: float, path_key: str) -> None:
        """
        S5 VELOCITY ENGINE - Track profit velocity and acceleration for $1M path.
        
        The Math:
        - Velocity (v) = Î”profit / Î”time ($/hour)
        - Acceleration (a) = Î”velocity / Î”time ($/hourÂ²)
        - Time to Million = (TARGET - current) / velocity
        
        S5 Logic:
        1. Speed: Maximize conversion frequency
        2. Scale: Increase position sizes on winning paths
        3. Smart: Route to highest-velocity paths
        4. Systematic: Phase transitions at milestones
        5. Sustainable: Compound aggressively while managing risk
        """
        now = time.time()
        elapsed_hours = max((now - self.start_time) / 3600, 0.001)
        
        # Current velocity
        current_profit = self.conversion_metrics['net_conversion_profit']
        current_velocity = current_profit / elapsed_hours
        self.conversion_metrics['velocity_per_hour'] = current_velocity
        
        # Conversion frequency
        total_conv = self.conversion_metrics['total_conversions']
        self.conversion_metrics['conversions_per_hour'] = total_conv / elapsed_hours
        
        # Track velocity history for acceleration
        self.s5_state['velocity_history'].append({
            'ts': now,
            'velocity': current_velocity,
            'profit': current_profit,
        })
        
        # Calculate acceleration (change in velocity)
        if len(self.s5_state['velocity_history']) >= 2:
            recent = list(self.s5_state['velocity_history'])[-10:]  # Last 10 readings
            if len(recent) >= 2:
                v1 = recent[0]['velocity']
                v2 = recent[-1]['velocity']
                t1 = recent[0]['ts']
                t2 = recent[-1]['ts']
                dt = max((t2 - t1) / 3600, 0.001)  # Hours
                acceleration = (v2 - v1) / dt
                self.conversion_metrics['acceleration'] = acceleration
                self.s5_state['acceleration_history'].append({
                    'ts': now,
                    'acceleration': acceleration,
                })
        
        # Time to Million calculation
        remaining = self.TARGET_MILLION - current_profit
        if current_velocity > 0:
            ttm_hours = remaining / current_velocity
            self.conversion_metrics['time_to_million'] = ttm_hours
        else:
            self.conversion_metrics['time_to_million'] = float('inf')
        
        # Track path-specific velocity
        if path_key not in self.s5_state['path_velocity']:
            self.s5_state['path_velocity'][path_key] = {
                'profit': 0.0, 'count': 0, 'first_ts': now, 'velocity': 0.0
            }
        pv = self.s5_state['path_velocity'][path_key]
        pv['profit'] += net_profit
        pv['count'] += 1
        path_hours = max((now - pv['first_ts']) / 3600, 0.001)
        pv['velocity'] = pv['profit'] / path_hours
        
        # S5 Phase Transitions
        self._check_s5_phase_transition(current_profit, current_velocity)
    
    def _check_s5_phase_transition(self, profit: float, velocity: float) -> None:
        """
        S5 Phase Management - Automatic scaling based on progress.
        
        Phases:
        - BOOTSTRAP: $0 - $100 (learning paths)
        - GROWTH: $100 - $10,000 (optimizing velocity)
        - SCALE: $10,000 - $100,000 (parallel execution)
        - COMPOUND: $100,000 - $500,000 (aggressive compounding)
        - MILLION: $500,000 - $1,000,000 (final push)
        """
        old_phase = self.s5_state['phase']
        new_phase = old_phase
        
        if profit < 100:
            new_phase = 'BOOTSTRAP'
        elif profit < 10_000:
            new_phase = 'GROWTH'
        elif profit < 100_000:
            new_phase = 'SCALE'
        elif profit < 500_000:
            new_phase = 'COMPOUND'
        else:
            new_phase = 'MILLION'
        
        if new_phase != old_phase:
            self.s5_state['phase'] = new_phase
            self.s5_state['phase_transitions'].append({
                'from': old_phase,
                'to': new_phase,
                'profit': profit,
                'velocity': velocity,
                'ts': time.time(),
            })
            logger.info(f"ðŸš€ S5 PHASE TRANSITION: {old_phase} â†’ {new_phase} | Profit: ${profit:.2f} | Velocity: ${velocity:.2f}/hr")
    
    def get_conversion_stats(self) -> Dict[str, Any]:
        """Get detailed conversion statistics for monitoring."""
        elapsed_hours = max((time.time() - self.start_time) / 3600, 0.001)
        elapsed_days = elapsed_hours / 24
        
        total = self.conversion_metrics['total_conversions']
        success = self.conversion_metrics['successful_conversions']
        
        # Calculate daily rate
        daily_conversions = total / elapsed_days if elapsed_days > 0 else 0
        daily_profit = self.conversion_metrics['net_conversion_profit'] / elapsed_days if elapsed_days > 0 else 0
        
        # Get top performing paths
        path_perf = self.conversion_metrics['path_performance']
        top_paths = sorted(
            [(k, v) for k, v in path_perf.items()],
            key=lambda x: x[1]['avg_profit'],
            reverse=True
        )[:5]
        
        # Get top performing assets
        asset_perf = self.conversion_metrics['asset_performance']
        top_assets = sorted(
            [(k, v) for k, v in asset_perf.items()],
            key=lambda x: x[1]['profit'],
            reverse=True
        )[:10]
        
        return {
            'total_conversions': total,
            'successful_conversions': success,
            'failed_conversions': self.conversion_metrics['failed_conversions'],
            'success_rate': success / total if total > 0 else 0,
            'net_conversion_profit': self.conversion_metrics['net_conversion_profit'],
            'total_fees': self.conversion_metrics['total_conversion_fees'],
            'profit_rate_per_hour': self.conversion_metrics['conversion_profit_rate_per_hour'],
            'profit_rate_per_day': self.conversion_metrics['conversion_profit_rate_per_hour'] * 24,
            'daily_conversions': daily_conversions,
            'daily_profit': daily_profit,
            'best_path': self.conversion_metrics['best_path'],
            'best_path_profit': self.conversion_metrics['best_path_profit'],
            'worst_path': self.conversion_metrics['worst_path'],
            'worst_path_loss': self.conversion_metrics['worst_path_loss'],
            'top_paths': top_paths,
            'top_assets': top_assets,
            'goals': self.conversion_goals,
            'goal_progress': {
                'daily_conversions': daily_conversions / self.conversion_goals['target_daily_conversions'],
                'daily_profit': daily_profit / self.conversion_goals['target_daily_profit'],
                'to_million': self.conversion_metrics['net_conversion_profit'] / self.TARGET_MILLION,
            },
            # S5 METRICS
            's5': {
                'phase': self.s5_state['phase'],
                'velocity': self.conversion_metrics.get('velocity_per_hour', 0),
                'acceleration': self.conversion_metrics.get('acceleration', 0),
                'time_to_million': self.conversion_metrics.get('time_to_million', float('inf')),
                'conversions_per_hour': self.conversion_metrics.get('conversions_per_hour', 0),
                'phase_transitions': len(self.s5_state.get('phase_transitions', [])),
                'top_velocity_paths': self._get_top_velocity_paths(5),
            },
        }
    
    def _get_top_velocity_paths(self, n: int = 5) -> List[Dict]:
        """Get the N highest velocity paths."""
        pv = self.s5_state.get('path_velocity', {})
        sorted_paths = sorted(
            [(k, v) for k, v in pv.items()],
            key=lambda x: x[1]['velocity'],
            reverse=True
        )[:n]
        return [
            {'path': p[0], 'velocity': p[1]['velocity'], 'count': p[1]['count'], 'profit': p[1]['profit']}
            for p in sorted_paths
        ]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ðŸ”¥ðŸ”¥ðŸ”¥ S5 AGGRESSIVE OPTIMIZATION ENGINE ðŸ”¥ðŸ”¥ðŸ”¥
    # Speed Ã— Scale Ã— Smart Ã— Systematic Ã— Sustainable = $1,000,000
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def s5_calculate_optimal_size(self, estimated_profit: float, confidence: float, 
                                   available_balance: float) -> float:
        """
        S5 Kelly-based position sizing for aggressive compounding.
        
        Kelly Formula: f* = (bp - q) / b
        Where:
            b = net odds (estimated_profit / risk)
            p = probability of winning (confidence)
            q = probability of losing (1 - p)
            f* = fraction of bankroll to bet
        
        We use fractional Kelly (S5_KELLY_FRACTION) for safety while still being aggressive.
        """
        if estimated_profit <= 0 or confidence <= 0:
            return 0.0
        
        # Calculate Kelly fraction
        # Assume risk = potential loss (conservative estimate: 2x the profit we expect)
        risk = max(estimated_profit * 2, 0.001)
        b = estimated_profit / risk  # Net odds
        p = confidence
        q = 1 - p
        
        kelly_fraction = (b * p - q) / b if b > 0 else 0
        kelly_fraction = max(0, kelly_fraction)  # Can't be negative
        
        # Apply S5 Kelly fraction (conservative Kelly)
        optimal_fraction = kelly_fraction * self.S5_KELLY_FRACTION
        
        # S5 Phase-based scaling
        phase_multipliers = {
            'BOOTSTRAP': 0.5,   # Conservative in early phase
            'GROWTH': 0.8,      # Building momentum
            'SCALE': 1.0,       # Full speed
            'COMPOUND': 1.2,    # Aggressive compounding
            'MILLION': 1.5      # Maximum aggression
        }
        phase_mult = phase_multipliers.get(self.s5_state['phase'], 1.0)
        optimal_fraction *= phase_mult
        
        # Velocity boost: if we're making money fast, size up
        velocity = self.conversion_metrics.get('velocity_per_hour', 0)
        if velocity > self.S5_VELOCITY_THRESHOLD:
            velocity_boost = min(velocity / self.S5_VELOCITY_THRESHOLD, 2.0)
            optimal_fraction *= (1 + (velocity_boost - 1) * 0.3)  # 30% of velocity boost
        
        # Cap at reasonable limits
        optimal_fraction = min(optimal_fraction, 0.15)  # Max 15% per trade
        
        # Calculate actual size
        optimal_size = available_balance * optimal_fraction
        
        return round(optimal_size, 8)
    
    def s5_adaptive_labyrinth_score(self, path_key: str, base_profit: float) -> float:
        """
        Score a labyrinth path based on S5 metrics.
        Higher scores = prioritize this path.
        
        âš¡ SPEED OPTIMIZED: Uses caching for hot paths.
        
        Factors:
        - Historical velocity on this path
        - Win rate on this path
        - Recent acceleration
        - Phase-appropriate scaling
        """
        # âš¡ Check cache first
        now = time.time()
        cached = self._s5_score_cache.get(path_key)
        if cached and (now - cached[1]) < self.S5_CACHE_TTL:
            # Use cached score, adjust for new base_profit
            cached_base = cached[2] if len(cached) > 2 else 1.0
            if cached_base > 0:
                return cached[0] * (base_profit / cached_base)
        
        score = base_profit * 1000  # Base score from profit
        
        # Path velocity bonus
        path_data = self.s5_state['path_velocity'].get(path_key, {})
        path_velocity = path_data.get('velocity', 0)
        if path_velocity > 0:
            score += path_velocity * 10  # Velocity bonus
        
        # Win rate bonus from path_performance
        path_perf = self.conversion_metrics['path_performance'].get(path_key, {})
        if path_perf:
            total = path_perf.get('wins', 0) + path_perf.get('losses', 0)
            if total >= 3:
                win_rate = path_perf.get('wins', 0) / total
                score *= (0.5 + win_rate)  # 50%-150% based on win rate
            
            # âš¡ Mark hot paths
            if total >= self.S5_HOT_PATH_THRESHOLD:
                self._s5_hot_paths.add(path_key)
        
        # Acceleration bonus: reward paths that are accelerating
        acceleration = self.conversion_metrics.get('acceleration', 0)
        if acceleration > 0:
            score *= (1 + acceleration * 0.01)  # 1% per $/hourÂ² acceleration
        
        # Phase multiplier
        phase_scores = {
            'BOOTSTRAP': 0.8,
            'GROWTH': 1.0,
            'SCALE': 1.2,
            'COMPOUND': 1.5,
            'MILLION': 2.0
        }
        score *= phase_scores.get(self.s5_state['phase'], 1.0)
        
        # âš¡ Cache the score
        self._s5_score_cache[path_key] = (score, now, base_profit)
        
        return score
    
    def s5_flush_batch_updates(self):
        """âš¡ Flush pending batch updates for speed optimization."""
        if not self._s5_pending_updates:
            return
        
        # Process all pending updates in one batch
        for update in self._s5_pending_updates:
            path_key = update['path_key']
            profit = update['profit']
            success = update['success']
            self.s5_update_labyrinth_cache(path_key, profit, success)
        
        self._s5_pending_updates.clear()
        self._s5_last_batch_time = time.time()
        
    def s5_queue_update(self, path_key: str, profit: float, success: bool):
        """âš¡ Queue an update for batch processing."""
        self._s5_pending_updates.append({
            'path_key': path_key,
            'profit': profit,
            'success': success
        })
        
        # Auto-flush if batch is full or time threshold reached
        if (len(self._s5_pending_updates) >= self.S5_BATCH_SIZE or 
            time.time() - self._s5_last_batch_time > 5):  # 5 second max wait
            self.s5_flush_batch_updates()
    
    def s5_get_hot_paths(self) -> List[str]:
        """âš¡ Get list of hot (frequently used) paths."""
        return list(self._s5_hot_paths)
    
    def s5_rank_conversion_paths(self, paths: List[Dict]) -> List[Dict]:
        """
        Rank conversion paths by S5 adaptive score.
        Input: List of dicts with 'from', 'to', 'estimated_profit', etc.
        Output: Sorted list with 's5_score' added, highest first.
        """
        scored = []
        for path in paths:
            path_key = f"{path.get('from', '?')}â†’{path.get('to', '?')}"
            estimated_profit = path.get('estimated_profit', 0)
            
            s5_score = self.s5_adaptive_labyrinth_score(path_key, estimated_profit)
            path_copy = dict(path)
            path_copy['s5_score'] = s5_score
            path_copy['path_key'] = path_key
            scored.append(path_copy)
        
        # Sort by S5 score descending
        scored.sort(key=lambda x: x['s5_score'], reverse=True)
        return scored
    
    def s5_get_optimal_conversions(self, available_paths: List[Dict], 
                                    max_conversions: int = 10) -> List[Dict]:
        """
        Get the optimal set of conversions to execute based on S5 math.
        
        This is the BRAIN of S5 - it decides WHAT to convert and HOW MUCH.
        """
        if not available_paths:
            return []
        
        # Rank all paths
        ranked = self.s5_rank_conversion_paths(available_paths)
        
        # Filter paths that meet minimum thresholds
        min_profit = self.conversion_goals['min_profit_per_conversion']
        viable = [p for p in ranked if p.get('estimated_profit', 0) >= min_profit]
        
        # Select top N paths
        selected = viable[:max_conversions]
        
        # Log S5 selection
        if selected:
            logger.info(f"ðŸ”¥ S5 OPTIMAL: Selected {len(selected)} conversions, "
                       f"top score={selected[0]['s5_score']:.2f}, "
                       f"phase={self.s5_state['phase']}")
        
        return selected
    
    def s5_update_labyrinth_cache(self, path_key: str, profit: float, success: bool):
        """
        Update the adaptive labyrinth cache with new performance data.
        This makes the labyrinth LEARN and ADAPT.
        """
        cache = self.s5_state['optimal_paths_cache']
        
        if path_key not in cache:
            cache[path_key] = {
                'total_profit': 0.0,
                'executions': 0,
                'successes': 0,
                'failures': 0,
                'avg_profit': 0.0,
                'score': 0.0,
                'last_used': time.time()
            }
        
        cache[path_key]['executions'] += 1
        cache[path_key]['total_profit'] += profit
        cache[path_key]['last_used'] = time.time()
        
        if success:
            cache[path_key]['successes'] += 1
        else:
            cache[path_key]['failures'] += 1
        
        # Update averages
        cache[path_key]['avg_profit'] = (
            cache[path_key]['total_profit'] / cache[path_key]['executions']
        )
        
        # Update score
        win_rate = cache[path_key]['successes'] / max(cache[path_key]['executions'], 1)
        cache[path_key]['score'] = cache[path_key]['avg_profit'] * win_rate
        
        # Prune old/bad paths from cache (keep cache efficient)
        if len(cache) > 500:  # Max 500 cached paths
            # Remove paths with negative avg profit or not used in 24h
            cutoff = time.time() - 86400
            to_remove = [
                k for k, v in cache.items()
                if v['avg_profit'] < 0 or v['last_used'] < cutoff
            ]
            for k in to_remove[:100]:  # Remove up to 100 bad paths
                del cache[k]
    
    def s5_get_time_to_million(self) -> Dict[str, Any]:
        """
        Calculate detailed time-to-million metrics.
        """
        current_profit = self.conversion_metrics['net_conversion_profit']
        velocity = self.conversion_metrics.get('velocity_per_hour', 0)
        acceleration = self.conversion_metrics.get('acceleration', 0)
        
        remaining = self.TARGET_MILLION - current_profit
        
        # Linear estimate
        ttm_linear = remaining / velocity if velocity > 0 else float('inf')
        
        # Accelerated estimate (with current acceleration continuing)
        # Using: distance = v*t + 0.5*a*t^2, solve for t
        # t = (-v + sqrt(v^2 + 2*a*remaining)) / a
        ttm_accelerated = float('inf')
        if acceleration > 0:
            discriminant = velocity**2 + 2 * acceleration * remaining
            if discriminant >= 0:
                ttm_accelerated = (-velocity + discriminant**0.5) / acceleration
                ttm_accelerated = max(0, ttm_accelerated)
        
        # Progress percentage
        progress_pct = (current_profit / self.TARGET_MILLION) * 100
        
        return {
            'current_profit': current_profit,
            'target': self.TARGET_MILLION,
            'remaining': remaining,
            'progress_pct': progress_pct,
            'velocity_per_hour': velocity,
            'acceleration': acceleration,
            'ttm_hours_linear': ttm_linear,
            'ttm_days_linear': ttm_linear / 24 if ttm_linear != float('inf') else float('inf'),
            'ttm_hours_accelerated': ttm_accelerated,
            'ttm_days_accelerated': ttm_accelerated / 24 if ttm_accelerated != float('inf') else float('inf'),
            'phase': self.s5_state['phase'],
        }
    
    def s5_summary(self) -> str:
        """
        Get a human-readable S5 progress summary.
        """
        ttm = self.s5_get_time_to_million()
        
        phase_emoji = {
            'BOOTSTRAP': 'ðŸŒ±',
            'GROWTH': 'ðŸŒ¿',
            'SCALE': 'ðŸŒ³',
            'COMPOUND': 'ðŸ”¥',
            'MILLION': 'ðŸ’Ž'
        }
        emoji = phase_emoji.get(ttm['phase'], 'ðŸ“Š')
        
        summary = f"""
{emoji} S5 MILLION DOLLAR TRACKER {emoji}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ’° Current:    ${ttm['current_profit']:,.2f}
ðŸŽ¯ Target:     ${ttm['target']:,.0f}
ðŸ“Š Progress:   {ttm['progress_pct']:.4f}%
ðŸš€ Velocity:   ${ttm['velocity_per_hour']:.2f}/hour
âš¡ Accel:      ${ttm['acceleration']:.4f}/hourÂ²
â±ï¸ TTM:        {ttm['ttm_days_linear']:.1f} days (linear)
ðŸ”¥ TTM Accel:  {ttm['ttm_days_accelerated']:.1f} days (accelerated)
ðŸ“ˆ Phase:      {ttm['phase']}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        return summary
    
    def should_convert(self, from_asset: str, to_asset: str, estimated_profit: float, hops: int = 1) -> bool:
        """
        Decide if a conversion is worth making based on profit potential.
        Uses historical performance data to make intelligent decisions.
        """
        # Must meet minimum profit threshold
        min_profit = self.conversion_goals['min_profit_per_conversion']
        if estimated_profit < min_profit:
            return False
        
        # Check path performance history
        path_key = f"{from_asset}â†’{to_asset}"
        path_perf = self.conversion_metrics['path_performance'].get(path_key)
        
        if path_perf:
            # If this path has historically lost money, require higher profit estimate
            if path_perf['avg_profit'] < 0:
                required_profit = min_profit * 3  # 3x threshold for historically bad paths
                if estimated_profit < required_profit:
                    return False
            
            # If win rate is low, require higher confidence
            total = path_perf['wins'] + path_perf['losses']
            if total >= 5:  # Enough history
                win_rate = path_perf['wins'] / total
                if win_rate < 0.5 and estimated_profit < min_profit * 2:
                    return False
        
        # Multi-hop penalty: each additional hop increases fee risk
        max_fee_per_hop = self.conversion_goals['max_single_hop_fee_pct']
        estimated_total_fee = max_fee_per_hop * hops
        if estimated_total_fee > estimated_profit * 0.5:  # Fees would eat >50% of profit
            return False
        
        return True
    
    def get_best_conversion_path(self, from_asset: str, available_targets: List[str]) -> Optional[str]:
        """
        Recommend the best conversion target based on historical performance.
        """
        best_target = None
        best_score = float('-inf')
        
        for target in available_targets:
            path_key = f"{from_asset}â†’{target}"
            path_perf = self.conversion_metrics['path_performance'].get(path_key)
            
            if path_perf and path_perf['count'] >= 3:
                # Score = avg_profit * win_rate
                total = path_perf['wins'] + path_perf['losses']
                win_rate = path_perf['wins'] / total if total > 0 else 0.5
                score = path_perf['avg_profit'] * win_rate
            else:
                # Unknown path: neutral score
                score = 0.0
            
            if score > best_score:
                best_score = score
                best_target = target
        
        return best_target

    def get_growth_stats(self) -> Dict[str, Any]:
        """
        Get growth statistics - how fast are we achieving THE GOAL?
        """
        current_equity = self.get_total_equity()
        elapsed_hours = max((time.time() - self.start_time) / 3600, 0.001)
        elapsed_days = elapsed_hours / 24
        
        return {
            "one_goal": self.ONE_GOAL,
            "starting_equity": self.starting_equity,
            "current_equity": current_equity,
            "target_equity": self.target_equity,
            "net_profit_total": self.net_profit_total,
            "profit_rate_per_hour": self.profit_rate_per_hour,
            "profit_rate_per_day": self.profit_rate_per_hour * 24,
            "growth_percentage": ((current_equity - self.starting_equity) / max(self.starting_equity, 1)) * 100,
            "elapsed_hours": elapsed_hours,
            "elapsed_days": elapsed_days,
            "peak_equity": self.peak_equity,
            "compound_rate": self.COMPOUND_RATE,
            "aggression": self.GROWTH_AGGRESSION
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # â³ðŸ”® TIMELINE ORACLE INTEGRATION - 7-Day Future Validation
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_consensus(self, symbol: str, action: str = None) -> float:
        """
        Get hive consensus for a symbol/action pair.
        
        This is used by the Timeline Oracle to validate timeline branches.
        Returns confidence score 0.0 to 1.0.
        """
        # Check if we have queen neuron activation
        if self.queen_neuron:
            base_consensus = (self.queen_neuron.activation + 1) / 2  # Normalize from [-1,1] to [0,1]
        else:
            base_consensus = 0.5
        
        # Check path performance if action implies a direction
        if action:
            path_perf = self.conversion_metrics.get('path_performance', {})
            # Look for any path involving this symbol
            symbol_performance = []
            for path_key, perf in path_perf.items():
                if symbol.upper() in path_key.upper():
                    total = perf.get('wins', 0) + perf.get('losses', 0)
                    if total > 0:
                        win_rate = perf.get('wins', 0) / total
                        symbol_performance.append(win_rate)
            
            if symbol_performance:
                path_consensus = sum(symbol_performance) / len(symbol_performance)
                # Blend with base consensus
                base_consensus = (base_consensus + path_consensus) / 2
        
        return min(1.0, max(0.0, base_consensus))
    
    def query(self, query_string: str) -> Optional[Dict[str, Any]]:
        """
        Query the mycelium network for consensus on a specific topic.
        
        Used by Timeline Oracle for hive intelligence queries.
        Query format: "FROM:TO" or "SYMBOL"
        """
        if ':' in query_string:
            # Conversion path query
            parts = query_string.split(':')
            from_asset, to_asset = parts[0], parts[1]
            path_key = f"{from_asset}â†’{to_asset}"
            
            path_perf = self.conversion_metrics.get('path_performance', {}).get(path_key)
            if path_perf:
                total = path_perf.get('wins', 0) + path_perf.get('losses', 0)
                win_rate = path_perf.get('wins', 0) / total if total > 0 else 0.5
                return {
                    'consensus': win_rate,
                    'confidence': min(total / 10, 1.0),  # More history = more confidence
                    'path_key': path_key,
                    'avg_profit': path_perf.get('avg_profit', 0),
                    'executions': total
                }
        
        # Symbol query - aggregate all paths involving this symbol
        symbol = query_string.upper()
        relevant_paths = []
        for path_key, perf in self.conversion_metrics.get('path_performance', {}).items():
            if symbol in path_key.upper():
                total = perf.get('wins', 0) + perf.get('losses', 0)
                if total > 0:
                    relevant_paths.append({
                        'path_key': path_key,
                        'win_rate': perf.get('wins', 0) / total,
                        'avg_profit': perf.get('avg_profit', 0),
                        'executions': total
                    })
        
        if relevant_paths:
            avg_win_rate = sum(p['win_rate'] for p in relevant_paths) / len(relevant_paths)
            total_executions = sum(p['executions'] for p in relevant_paths)
            return {
                'consensus': avg_win_rate,
                'confidence': min(total_executions / 20, 1.0),
                'symbol': symbol,
                'paths_count': len(relevant_paths),
                'total_executions': total_executions
            }
        
        return None
    
    def get_health(self) -> Dict[str, Any]:
        """
        Get overall health/status of the mycelium network.
        
        Used by Timeline Oracle for environment signal.
        """
        current_equity = self.get_total_equity()
        growth_stats = self.get_growth_stats()
        
        # Health score based on multiple factors
        health_factors = []
        
        # Factor 1: Equity growth
        growth_pct = growth_stats.get('growth_percentage', 0)
        health_factors.append(min(1.0, max(0.0, (growth_pct + 10) / 20)))  # -10% to +10% mapped to 0-1
        
        # Factor 2: Win rate
        metrics = self.conversion_metrics
        total_conv = metrics.get('successful_conversions', 0) + metrics.get('failed_conversions', 0)
        if total_conv > 0:
            win_rate = metrics.get('successful_conversions', 0) / total_conv
            health_factors.append(win_rate)
        else:
            health_factors.append(0.5)  # Neutral if no history
        
        # Factor 3: Velocity
        velocity = metrics.get('velocity_per_hour', 0)
        health_factors.append(min(1.0, velocity / 50))  # $50/hr = perfect health
        
        # Factor 4: Phase progress
        phase_scores = {'BOOTSTRAP': 0.2, 'GROWTH': 0.4, 'SCALE': 0.6, 'COMPOUND': 0.8, 'MILLION': 1.0}
        phase_score = phase_scores.get(self.s5_state.get('phase', 'BOOTSTRAP'), 0.2)
        health_factors.append(phase_score)
        
        overall_health = sum(health_factors) / len(health_factors) if health_factors else 0.5
        
        return {
            'score': overall_health,
            'equity': current_equity,
            'growth_pct': growth_pct,
            'velocity': velocity,
            'phase': self.s5_state.get('phase', 'BOOTSTRAP'),
            'hives_count': len(self.hives),
            'active': True
        }
    
    def boost_signal_for_profit(self, base_signal: float, expected_profit: float) -> float:
        """
        Boost signals that lead to more profit.
        THE GOAL: If it makes money, DO MORE OF IT!
        """
        if expected_profit <= 0:
            return base_signal * 0.5  # Reduce signals that don't make money
        
        # Boost proportional to expected profit
        profit_boost = min(expected_profit / self.MIN_PROFIT_TARGET, 3.0)
        boosted = base_signal * (1 + (profit_boost * self.GROWTH_AGGRESSION))
        
        return min(boosted, 1.0)  # Cap at 1.0
    
    def should_take_trade(self, expected_net_profit: float, confidence: float) -> bool:
        """
        Simple decision: Does this trade grow net profit?
        If yes â†’ TAKE IT. If no â†’ SKIP IT.
        
        THE LOGIC:
        - Below $0.03 target â†’ NEVER take (not worth fees)
        - At $0.03 â†’ Need 70% confidence
        - At $0.05 â†’ Need 60% confidence (profit justifies risk)
        - At $0.10+ â†’ Need only 50% confidence (big profit = GO!)
        """
        # Must meet minimum profit target
        if expected_net_profit < self.MIN_PROFIT_TARGET:
            return False
        
        # Higher profit = lower confidence needed (more aggressive scaling)
        # Every $0.01 above min target reduces required confidence by 5%
        profit_above_min = expected_net_profit - self.MIN_PROFIT_TARGET
        confidence_reduction = profit_above_min * 5  # 5% per $0.01
        
        required_confidence = 0.7 - confidence_reduction
        required_confidence = max(required_confidence, 0.5)  # Never go below 50%
        
        return confidence >= required_confidence
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ðŸ”±ðŸ”® ENHANCED PROBABILITY NEXUS INTEGRATION ðŸ”®ðŸ”±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_enhanced_prediction(self, pair: str, candles: List[dict] = None, candle_idx: int = None) -> Dict[str, Any]:
        """
        Get prediction from Enhanced Probability Nexus with profit filter.
        
        Returns dict with:
        - direction: 'LONG', 'SHORT', 'NEUTRAL'
        - is_profitable: bool (has profitable exit within 15 candles)
        - confidence: float
        - optimal_hold: int (best hold time in candles)
        - expected_profit: float (expected profit %)
        """
        if not self.enhanced_nexus:
            return {
                'direction': 'NEUTRAL',
                'is_profitable': False,
                'confidence': 0,
                'optimal_hold': 0,
                'expected_profit': 0,
            }
        
        try:
            prediction, is_profitable, optimal_hold, expected_profit = \
                self.enhanced_nexus.predict_with_profit_filter(pair, candles, candle_idx)
            
            return {
                'direction': prediction.direction,
                'is_profitable': is_profitable,
                'confidence': prediction.confidence,
                'optimal_hold': optimal_hold,
                'expected_profit': expected_profit,
                'probability': prediction.probability,
                'factors': prediction.factors,
                'reason': prediction.reason,
            }
        except Exception as e:
            logger.warning(f"Enhanced prediction failed: {e}")
            return {
                'direction': 'NEUTRAL',
                'is_profitable': False,
                'confidence': 0,
                'optimal_hold': 0,
                'expected_profit': 0,
            }
    
    def execute_enhanced_trade(self, pair: str, direction: str, entry_price: float, 
                                exit_price: float, confidence: float) -> Dict[str, Any]:
        """
        Execute a trade through the Enhanced Nexus compounding engine.
        
        This uses Kelly-style position sizing and tracks compounding growth.
        """
        if not self.enhanced_nexus:
            return {'success': False, 'reason': 'Enhanced Nexus not available'}
        
        try:
            trade = self.enhanced_nexus.execute_trade(
                pair=pair,
                direction=direction,
                entry_price=entry_price,
                exit_price=exit_price,
                confidence=confidence
            )
            
            # Record the profit to Mycelium tracking
            self.record_trade_profit(trade['net_pnl'], trade)
            
            return {
                'success': True,
                'trade': trade,
                'win_rate': self.enhanced_nexus.get_win_rate(),
                'total_pnl': self.enhanced_nexus.total_pnl,
                'balance': self.enhanced_nexus.compounding.balance,
            }
        except Exception as e:
            logger.error(f"Enhanced trade execution failed: {e}")
            return {'success': False, 'reason': str(e)}
    
    def get_enhanced_nexus_status(self) -> Dict[str, Any]:
        """Get the status of the Enhanced Probability Nexus"""
        if not self.enhanced_nexus:
            return {'available': False}
        
        try:
            report = self.enhanced_nexus.get_performance_report()
            return {
                'available': True,
                'win_rate': report.get('win_rate', 0),
                'total_trades': report.get('total_trades', 0),
                'balance': report.get('current_balance', 0),
                'total_pnl': report.get('total_pnl', 0),
                'return_pct': report.get('total_return_pct', 0),
                'pairs_tracked': report.get('pairs_tracked', 0),
            }
        except:
            return {'available': True, 'status': 'initializing'}
    
    def sync_nexus_balance(self, actual_balance: float):
        """Sync the Enhanced Nexus balance with actual account balance"""
        if self.enhanced_nexus and self.enhanced_nexus.compounding:
            self.enhanced_nexus.compounding.balance = actual_balance
            self.enhanced_nexus.compounding.starting_balance = actual_balance
            logger.info(f"ðŸ”± Enhanced Nexus synced to balance: ${actual_balance:,.2f}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ðŸ„ FULL MESH SIGNAL NETWORK - SMALLâ†”BIG, UPâ†”DOWN, LEFTâ†”RIGHT ðŸ„
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_queen_signal(self, market_data: Dict = None) -> float:
        """
        Get the Queen Neuron's aggregated signal.
        This is the MASTER SIGNAL that combines all hives and agents.
        
        ðŸŒŒ ENHANCED: Now includes Stargate Protocol coherence!
        """
        if not self.hives:
            return 0.0
        
        # If market data provided, do a full step first
        if market_data:
            result = self.step(market_data)
            base_signal = result.get('queen_signal', 0.0)
        else:
            # Otherwise, get cached queen signal from last step
            # Aggregate all hive signals through synapses
            hive_signals = []
            for hive in self.hives:
                # Get aggregate signal from hive's agents
                agent_signals = [a.last_signal for a in hive.agents if hasattr(a, 'last_signal')]
                if agent_signals:
                    hive_signal = sum(agent_signals) / len(agent_signals)
                    hive_signals.append(hive_signal)
            
            if not hive_signals:
                return 0.0
            
            # Queen aggregates all signals
            transmitted = []
            for i, signal in enumerate(hive_signals):
                if i < len(self.hive_synapses):
                    transmitted.append(self.hive_synapses[i].transmit(signal))
                else:
                    transmitted.append(signal)
            
            base_signal = self.queen_neuron.activate(transmitted)
        
        # ðŸŒŒ STARGATE PROTOCOL INTEGRATION - Enhance signal with quantum coherence
        stargate_boost = self._get_stargate_coherence_boost()
        if stargate_boost != 0.0:
            # Apply boost as a small adjustment (Â±0.1 max)
            adjusted_signal = base_signal + (stargate_boost * 0.1)
            # Clamp to [-1, 1]
            adjusted_signal = max(-1.0, min(1.0, adjusted_signal))
            return adjusted_signal
        
        return base_signal
    
    def _get_stargate_coherence_boost(self) -> float:
        """
        ðŸŒŒ Get coherence boost from Stargate Protocol.
        Returns: -1 to +1 boost factor based on quantum coherence.
        """
        boost = 0.0
        
        # Check if Queen has Stargate systems wired
        if hasattr(self, 'queen_hive_mind') and self.queen_hive_mind:
            queen = self.queen_hive_mind
            
            # Get Stargate Engine coherence
            if hasattr(queen, 'stargate_engine') and queen.stargate_engine:
                try:
                    status = queen.stargate_engine.get_status()
                    global_coherence = status.get('global_coherence', 0.0)
                    standing_wave = status.get('standing_wave_intensity', 0.0)
                    
                    # High coherence = positive boost, low = negative
                    coherence_factor = (global_coherence - 0.5) * 2  # Map 0.5-1.0 to 0-1
                    wave_factor = (standing_wave - 0.5) * 2
                    
                    boost += (coherence_factor * 0.6 + wave_factor * 0.4)
                except Exception:
                    pass
            
            # Get Quantum Mirror Scanner coherence
            if hasattr(queen, 'quantum_mirror_scanner') and queen.quantum_mirror_scanner:
                try:
                    status = queen.quantum_mirror_scanner.get_status()
                    scanner_coherence = status.get('global_coherence', 0.5)
                    
                    # PHI-aligned branches give extra boost
                    if scanner_coherence >= 0.618:  # Golden ratio threshold
                        boost += 0.2
                    else:
                        boost += (scanner_coherence - 0.5) * 0.5
                except Exception:
                    pass
            
            # Get Timeline Anchor ready status
            if hasattr(queen, 'timeline_validator') and queen.timeline_validator:
                try:
                    status = queen.timeline_validator.get_status()
                    execution_ready = status.get('execution_ready', 0)
                    
                    if execution_ready > 0:
                        boost += 0.15 * min(execution_ready, 3)  # Cap at 3 ready anchors
                except Exception:
                    pass
        
        # Clamp boost to [-1, 1]
        return max(-1.0, min(1.0, boost))
    
    def wire_stargate_protocol(self, stargate_engine, quantum_scanner=None, timeline_validator=None) -> bool:
        """
        ðŸŒŒ Wire Stargate Protocol systems directly to Mycelium.
        This enables quantum coherence to influence the neural network.
        """
        try:
            self.stargate_engine = stargate_engine
            self.quantum_mirror_scanner = quantum_scanner
            self.timeline_anchor_validator = timeline_validator
            
            logger.info("ðŸ„ðŸŒŒ Mycelium Network: Stargate Protocol WIRED")
            logger.info(f"   â­ Stargate Engine: {'âœ…' if stargate_engine else 'âŒ'}")
            logger.info(f"   ðŸ”® Quantum Scanner: {'âœ…' if quantum_scanner else 'âŒ'}")
            logger.info(f"   âš“ Timeline Validator: {'âœ…' if timeline_validator else 'âŒ'}")
            
            return True
        except Exception as e:
            logger.error(f"Failed to wire Stargate Protocol to Mycelium: {e}")
            return False
    
    
    def receive_queen_wisdom(self, wisdom: Any) -> None:
        """
        ðŸ‘‘ Receive wisdom from the Queen Hive Mind.
        This adjusts the Queen Neuron's bias based on prophetic guidance.
        """
        if wisdom is None:
            return
        
        self.queen_wisdom_cache = wisdom
        
        # Extract direction and confidence from wisdom
        direction = getattr(wisdom, 'direction', None) or wisdom.get('direction', 'NEUTRAL') if isinstance(wisdom, dict) else 'NEUTRAL'
        confidence = getattr(wisdom, 'confidence', 0.5) or wisdom.get('confidence', 0.5) if isinstance(wisdom, dict) else 0.5
        
        # Adjust queen neuron bias based on wisdom
        # Positive bias = bullish tendency, negative = bearish
        if direction == 'BULLISH':
            bias_adjustment = confidence * 0.3  # Gentle influence
        elif direction == 'BEARISH':
            bias_adjustment = -confidence * 0.3
        else:
            bias_adjustment = 0.0
        
        # Apply to queen neuron
        self.queen_neuron.bias = bias_adjustment
        
        logger.debug(f"ðŸ‘‘ Mycelium received Queen wisdom: {direction} ({confidence:.0%}) â†’ bias={bias_adjustment:.3f}")
    
    def connect_to_queen(self, queen: Any) -> bool:
        """
        ðŸ‘‘ Connect this Mycelium Network to the Queen Hive Mind.
        The Queen becomes the central consciousness.
        """
        try:
            self.queen_hive_mind = queen
            logger.info("ðŸ‘‘ðŸ„ Mycelium Network CONNECTED to Queen Hive Mind")
            return True
        except Exception as e:
            logger.error(f"Failed to connect to Queen: {e}")
            return False
    
    def get_network_coherence(self) -> float:
        """
        Get network coherence - how aligned are all agents?
        1.0 = All agents agree perfectly
        0.0 = Maximum disagreement
        """
        if not self.hives:
            return 0.5
        
        all_signals = []
        for hive in self.hives:
            for agent in hive.agents:
                if hasattr(agent, 'last_signal'):
                    all_signals.append(agent.last_signal)
        
        if len(all_signals) < 2:
            return 0.5
        
        # Coherence = 1 - variance of signals
        mean_signal = sum(all_signals) / len(all_signals)
        variance = sum((s - mean_signal) ** 2 for s in all_signals) / len(all_signals)
        
        # Normalize variance to [0, 1] range (assuming signals in [-1, 1])
        normalized_variance = min(variance / 1.0, 1.0)
        coherence = 1.0 - normalized_variance
        
        return max(0.0, min(1.0, coherence))
    
    def broadcast_signal(self, signal_type: str, data: Dict) -> None:
        """
        Broadcast a signal to ALL connected systems.
        Direction: OUTWARD (Mycelium â†’ All subsystems)
        """
        # Store for propagation
        broadcast = {
            'type': signal_type,
            'data': data,
            'timestamp': time.time(),
            'source': 'mycelium_queen',
            'coherence': self.get_network_coherence(),
        }
        
        # Add to connection map for external systems to read
        if 'broadcasts' not in self.connection_map:
            self.connection_map['broadcasts'] = deque(maxlen=100)
        self.connection_map['broadcasts'].append(broadcast)
        
        # Propagate to all hives
        for hive in self.hives:
            hive.receive_broadcast(broadcast) if hasattr(hive, 'receive_broadcast') else None
    
    def receive_external_signal(self, source: str, signal: float, confidence: float = 0.5) -> None:
        """
        Receive a signal from an external system.
        Direction: INWARD (External â†’ Mycelium)
        
        Sources can be: probability_matrix, adaptive_learner, chess_brain, 
                       miner_brain, quantum_telescope, historical_wisdom, etc.
        """
        # Store external signal
        if 'external_signals' not in self.connection_map:
            self.connection_map['external_signals'] = {}
        
        self.connection_map['external_signals'][source] = {
            'signal': signal,
            'confidence': confidence,
            'timestamp': time.time()
        }
        
        # Blend into queen neuron bias (weighted by confidence)
        blend_weight = 0.1 * confidence  # Max 10% influence per source
        self.queen_neuron.bias = (1 - blend_weight) * self.queen_neuron.bias + blend_weight * signal
    
    def get_unified_signal(self, asset: str = None, include_external: bool = True) -> Dict:
        """
        Get a UNIFIED signal combining ALL sources:
        - Internal: Queen neuron, all hives, all agents
        - External: Probability, Adaptive, Historical, Chess, Quantum
        
        Returns comprehensive decision data.
        """
        # Internal signals
        queen_signal = self.get_queen_signal()
        coherence = self.get_network_coherence()
        
        # External signals
        external_signals = self.connection_map.get('external_signals', {})
        
        # Weight and combine
        signals = [queen_signal * coherence * 2]  # Queen has 2x weight
        confidences = [coherence]
        
        if include_external:
            for source, data in external_signals.items():
                # Only use recent signals (< 60 seconds old)
                if time.time() - data['timestamp'] < 60:
                    signals.append(data['signal'] * data['confidence'])
                    confidences.append(data['confidence'])
        
        if not signals:
            return {
                'signal': 0.0,
                'action': 'HOLD',
                'confidence': 0.5,
                'coherence': coherence,
                'sources': 0
            }
        
        # Unified signal = weighted average
        unified_signal = sum(signals) / len(signals)
        unified_confidence = sum(confidences) / len(confidences)
        
        # Determine action
        if unified_signal > 0.3 and unified_confidence > 0.5:
            action = 'BUY'
        elif unified_signal < -0.3 and unified_confidence > 0.5:
            action = 'SELL'
        else:
            action = 'HOLD'
        
        return {
            'signal': unified_signal,
            'action': action,
            'confidence': unified_confidence,
            'coherence': coherence,
            'queen_signal': queen_signal,
            'external_count': len(external_signals),
            'sources': len(signals)
        }
    
    def connect_subsystem(self, name: str, subsystem: Any) -> None:
        """
        Connect a subsystem to the Mycelium mesh.
        Enables bidirectional communication.
        """
        self.connection_map[name] = {
            'instance': subsystem,
            'connected_at': time.time(),
            'signals_sent': 0,
            'signals_received': 0
        }
        logger.info(f"ðŸ„ Mycelium: {name} connected to network")
    
    def propagate_to_all(self, message_type: str, payload: Dict) -> int:
        """
        Propagate a message to ALL connected subsystems.
        Returns count of systems that received the message.
        """
        count = 0
        for name, conn in self.connection_map.items():
            if isinstance(conn, dict) and 'instance' in conn:
                instance = conn['instance']
                if hasattr(instance, 'receive_mycelium_message'):
                    try:
                        instance.receive_mycelium_message(message_type, payload)
                        conn['signals_sent'] = conn.get('signals_sent', 0) + 1
                        count += 1
                    except Exception:
                        pass
        return count
    
    def get_mesh_status(self) -> Dict:
        """Get status of the full mesh network."""
        connected = []
        for name, conn in self.connection_map.items():
            if isinstance(conn, dict) and 'instance' in conn:
                connected.append({
                    'name': name,
                    'connected_at': conn.get('connected_at'),
                    'signals_sent': conn.get('signals_sent', 0),
                    'signals_received': conn.get('signals_received', 0)
                })
        
        return {
            'queen_signal': self.get_queen_signal(),
            'coherence': self.get_network_coherence(),
            'hive_count': len(self.hives),
            'agent_count': self.get_total_agents(),
            'connected_systems': connected,
            'external_signals': len(self.connection_map.get('external_signals', {})),
            'broadcasts_pending': len(self.connection_map.get('broadcasts', []))
        }

    def get_state(self) -> Dict[str, Any]:
        """Get full network state - INCLUDING THE GOAL METRICS AND CONVERSION STATS!"""
        growth = self.get_growth_stats()
        conversion_stats = self.get_conversion_stats()
        return {
            "timestamp": time.time(),
            "step": self.step_count,
            "one_goal": self.ONE_GOAL,
            "net_profit_total": growth["net_profit_total"],
            "profit_rate_per_day": growth["profit_rate_per_day"],
            "growth_percentage": growth["growth_percentage"],
            "total_hives": len(self.hives),
            "total_agents": self.get_total_agents(),
            "total_equity": self.get_total_equity(),
            "total_harvested": self.total_harvested,
            "generation": self.generation,
            "hives": [hive.get_metrics() for hive in self.hives],
            "split_events": self.split_events[-10:],  # Last 10 splits
            # ðŸ”„ CONVERSION METRICS
            "conversion_stats": {
                "total_conversions": conversion_stats["total_conversions"],
                "success_rate": conversion_stats["success_rate"],
                "net_conversion_profit": conversion_stats["net_conversion_profit"],
                "profit_rate_per_day": conversion_stats["profit_rate_per_day"],
                "best_path": conversion_stats["best_path"],
                "goal_progress": conversion_stats["goal_progress"],
            },
        }
    
    def display(self):
        """Display network status - ALWAYS SHOW THE GOAL!"""
        state = self.get_state()
        growth = self.get_growth_stats()
        
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ðŸŽ¯ ONE GOAL: {self.ONE_GOAL:^55} ðŸŽ¯  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ðŸ’° NET PROFIT: ${growth['net_profit_total']:>12.2f}  |  ðŸ“ˆ Rate: ${growth['profit_rate_per_day']:>8.2f}/day          â•‘
â•‘  ðŸ“Š Growth: {growth['growth_percentage']:>6.1f}%  |  Peak: ${growth['peak_equity']:>10.2f}                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        ðŸ„ MYCELIUM NEURAL NETWORK ðŸ„                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Step: {state['step']:<10}  Generation: {state['generation']:<5}                                â•‘
â•‘  Hives: {state['total_hives']:<10} Agents: {state['total_agents']:<5}                                    â•‘
â•‘  Total Equity: ${state['total_equity']:<15.4f}                                      â•‘
â•‘  Harvested: ${state['total_harvested']:<15.4f}                                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£""")
        
        for hive in state['hives'][:5]:  # Show first 5 hives
            stage_emoji = "ðŸŒ±" if hive['stage'] == 'growing' else \
                         "ðŸ”„" if hive['stage'] == 'ready_to_split' else "ðŸ†"
            print(f"â•‘  {stage_emoji} {hive['id']}: ${hive['equity']:.4f} ({hive['profit_multiplier']:.2f}x) [{hive['stage']}]")
        
        if len(state['hives']) > 5:
            print(f"â•‘  ... and {len(state['hives']) - 5} more hives")
        
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    def wire_hft_engine(self, hft_engine) -> bool:
        """Wire HFT engine to Mycelium network for fast neural path.
        
        This allows the Mycelium network to provide fast pattern recognition
        signals directly to the HFT engine for sub-10ms decision making.
        
        Returns:
            True if wiring succeeded, False otherwise.
        """
        if hft_engine is None:
            logger.warning("ðŸ„ Cannot wire HFT engine: None provided")
            return False
        
        try:
            self._hft_engine = hft_engine
            
            # Wire this network to the HFT engine's fast path
            if hasattr(hft_engine, 'wire_mycelium'):
                hft_engine.wire_mycelium(self)
            
            logger.info("ðŸ„âš¡ Mycelium WIRED to HFT Engine (fast neural path)")
            return True
        except Exception as e:
            logger.warning(f"ðŸ„ Failed to wire HFT engine: {e}")
            return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTEGRATION WITH AUREON NEXUS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_mycelium_for_nexus(initial_capital: float = 100.0) -> MyceliumNetwork:
    """
    Factory function to create a Mycelium Network integrated with Aureon Nexus.
    """
    return MyceliumNetwork(
        initial_capital=initial_capital,
        agents_per_hive=5,
        target_multiplier=2.0
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GLOBAL SINGLETON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_mycelium_instance: MyceliumNetwork = None

def get_mycelium(initial_capital: float = 100.0) -> MyceliumNetwork:
    """Get or create the global Mycelium Network instance."""
    global _mycelium_instance
    if _mycelium_instance is None:
        _mycelium_instance = MyceliumNetwork(
            initial_capital=initial_capital,
            agents_per_hive=5,
            target_multiplier=2.0
        )
    return _mycelium_instance


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN - Demo
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘     ðŸ„ AUREON MYCELIUM NEURAL NETWORK - DEMO ðŸ„                               â•‘
â•‘                                                                               â•‘
â•‘     "The underground network where everything connects"                        â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Create network with $100 initial capital
    network = MyceliumNetwork(initial_capital=100.0, agents_per_hive=5)
    
    # Simulate 50 trading steps
    for i in range(50):
        # Simulated market data
        market_data = {
            "price": 95000 + random.uniform(-500, 500),
            "momentum": random.uniform(-0.5, 0.5),
            "volatility": random.uniform(0.2, 0.8),
            "trend": random.uniform(-0.3, 0.3)
        }
        
        result = network.step(market_data)
        
        if i % 10 == 0:
            network.display()
    
    # Final state
    print("\nðŸ“Š FINAL STATE:")
    network.display()
    
    state = network.get_state()
    print(f"\nâœ… Simulation complete!")
    print(f"   Steps: {state['step']}")
    print(f"   Final Equity: ${state['total_equity']:.2f}")
    print(f"   ROI: {((state['total_equity'] - 100) / 100 * 100):.2f}%")
    print(f"   Hive Spawns: {len(state['split_events'])}")
